{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revChatGPT.V1 import Chatbot\n",
    "import json\n",
    "f = open('config.json')\n",
    "config = json.load(f)\n",
    "\n",
    "chatbot = Chatbot(config={\n",
    "  \"access_token\": config['access_token_lab']\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4_debug(chatbot, prompt, model=\"text-davinci-002-render-paid\"):\n",
    "    for data in chatbot.ask(prompt, conversation_id=None, parent_id=None, model=model):\n",
    "        response = data[\"message\"]\n",
    "    print(response)\n",
    "    # GPT4_clear_conversations(chatbot)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_debug(chatbot, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_config = {\n",
    "    \"dateset\": \"./data/\",\n",
    "    \"output\": \"predicted_sql.txt\"\n",
    "}\n",
    "# if sys.argv[1] == \"--dataset\" and sys.argv[3] == \"--output\":\n",
    "DATASET_SCHEMA = tmp_config['dateset']+\"tables.json\"\n",
    "DATASET = tmp_config['dateset']+\"dev.json\"\n",
    "OUTPUT_FILE = tmp_config['output']\n",
    "BEFORE_SQL = \"sql_norm_del_value_record_get_before.txt\"\n",
    "# else:\n",
    "#     raise Exception(\"Please use this format python CoT.py --dataset data/ --output predicted_sql.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(DATASET):\n",
    "  return pd.read_json(DATASET)\n",
    "\n",
    "def load_sql_txt(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    return [line.strip() for line in f]\n",
    "\n",
    "def hard_prompt_maker(test_sample_text,database,schema_links,sub_questions):\n",
    "  instruction = tmp2\n",
    "  instruction += \"# Use the intermediate representation and the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "  fields = find_fields_MYSQL_like(\"college_2\")\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(\"college_2\") + '\\n'\n",
    "  fields += find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  stepping = f'''\\nA: Let's think step by step. \"{test_sample_text}\" can be solved by knowing the answer to the following sub-question \"{sub_questions}\".'''\n",
    "  fields += \"\\n\"\n",
    "  prompt = instruction +fields + hard_prompt + 'Q: \"' + test_sample_text + '\"' + '\\nschema_links: ' + schema_links + stepping +'\\nThe SQL query for the sub-question\"'\n",
    "  return prompt\n",
    "def medium_prompt_maker(test_sample_text,database,schema_links):\n",
    "  instruction = tmp2\n",
    "  instruction += \"# Use the the schema links and Intermediate_representation to generate the SQL queries for each of the questions.\\n\"\n",
    "  fields = find_fields_MYSQL_like(\"college_2\")\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(\"college_2\") + '\\n'\n",
    "  fields += find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  fields += \"\\n\"\n",
    "  prompt = instruction +fields + medium_prompt + 'Q: \"' + test_sample_text + '\\nSchema_links: ' + schema_links + '\\nA: Let’s think step by step.'\n",
    "  return prompt\n",
    "def easy_prompt_maker(test_sample_text,database,schema_links):\n",
    "  instruction = \"# Use the the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "  fields = find_fields_MYSQL_like(\"college_2\")\n",
    "  fields += find_fields_MYSQL_like(database)\n",
    "  fields += \"\\n\"\n",
    "  prompt = instruction +fields + easy_prompt + 'Q: \"' + test_sample_text + '\\nSchema_links: ' + schema_links + '\\nSQL:'\n",
    "  return prompt\n",
    "def classification_prompt_maker(test_sample_text,database,schema_links):\n",
    "  instruction = \"# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\"\n",
    "  instruction += \"\\nif need nested queries: predict NESTED\\n\"\n",
    "  instruction += \"elif need JOIN and don't need nested queries: predict NON-NESTED\\n\"\n",
    "  instruction += \"elif don't need JOIN and don't need nested queries: predict EASY\\n\\n\"\n",
    "  fields = find_fields_MYSQL_like(\"college_2\")\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(\"college_2\") + '\\n'\n",
    "  fields += find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  fields += \"\\n\"\n",
    "  prompt = instruction + fields + classification_prompt + 'Q: \"' + test_sample_text + '\\nschema_links: ' + schema_links + '\\nA: Let’s think step by step.'\n",
    "  return prompt\n",
    "def schema_linking_prompt_maker(test_sample_text,database):\n",
    "  instruction = \"# Find the schema_links for generating SQL queries for each question based on the database schema and Foreign keys.\\n\"\n",
    "  fields = find_fields_MYSQL_like(database)\n",
    "  foreign_keys = \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  prompt = instruction + schema_linking_prompt + fields +foreign_keys+ 'Q: \"' + test_sample_text + \"\"\"\"\\nA: Let’s think step by step.\"\"\"\n",
    "  return prompt\n",
    "def find_foreign_keys_MYSQL_like(db_name):\n",
    "  df = spider_foreign[spider_foreign['Database name'] == db_name]\n",
    "  output = \"[\"\n",
    "  for index, row in df.iterrows():\n",
    "    output += row['First Table Name'] + '.' + row['First Table Foreign Key'] + \" = \" + row['Second Table Name'] + '.' + row['Second Table Foreign Key'] + ','\n",
    "  output= output[:-1] + \"]\"\n",
    "  return output\n",
    "def find_fields_MYSQL_like(db_name):\n",
    "  df = spider_schema[spider_schema['Database name'] == db_name]\n",
    "  df = df.groupby(' Table Name')\n",
    "  output = \"\"\n",
    "  for name, group in df:\n",
    "    output += \"Table \" +name+ ', columns = ['\n",
    "    for index, row in group.iterrows():\n",
    "      output += row[\" Field Name\"]+','\n",
    "    output = output[:-1]\n",
    "    output += \"]\\n\"\n",
    "  return output\n",
    "def find_primary_keys_MYSQL_like(db_name):\n",
    "  df = spider_primary[spider_primary['Database name'] == db_name]\n",
    "  output = \"[\"\n",
    "  for index, row in df.iterrows():\n",
    "    output += row['Table Name'] + '.' + row['Primary Key'] +','\n",
    "  output = output[:-1]\n",
    "  output += \"]\\n\"\n",
    "  return output\n",
    "def creatiing_schema(DATASET_JSON):\n",
    "    schema_df = pd.read_json(DATASET_JSON)\n",
    "    schema_df = schema_df.drop(['column_names','table_names'], axis=1)\n",
    "    schema = []\n",
    "    f_keys = []\n",
    "    p_keys = []\n",
    "    for index, row in schema_df.iterrows():\n",
    "        tables = row['table_names_original']\n",
    "        col_names = row['column_names_original']\n",
    "        col_types = row['column_types']\n",
    "        foreign_keys = row['foreign_keys']\n",
    "        primary_keys = row['primary_keys']\n",
    "        for col, col_type in zip(col_names, col_types):\n",
    "            index, col_name = col\n",
    "            if index == -1:\n",
    "                for table in tables:\n",
    "                    schema.append([row['db_id'], table, '*', 'text'])\n",
    "            else:\n",
    "                schema.append([row['db_id'], tables[index], col_name, col_type])\n",
    "        for primary_key in primary_keys:\n",
    "            index, column = col_names[primary_key]\n",
    "            p_keys.append([row['db_id'], tables[index], column])\n",
    "        for foreign_key in foreign_keys:\n",
    "            first, second = foreign_key\n",
    "            first_index, first_column = col_names[first]\n",
    "            second_index, second_column = col_names[second]\n",
    "            f_keys.append([row['db_id'], tables[first_index], tables[second_index], first_column, second_column])\n",
    "    spider_schema = pd.DataFrame(schema, columns=['Database name', ' Table Name', ' Field Name', ' Type'])\n",
    "    spider_primary = pd.DataFrame(p_keys, columns=['Database name', 'Table Name', 'Primary Key'])\n",
    "    spider_foreign = pd.DataFrame(f_keys,\n",
    "                        columns=['Database name', 'First Table Name', 'Second Table Name', 'First Table Foreign Key',\n",
    "                                 'Second Table Foreign Key'])\n",
    "    return spider_schema,spider_primary,spider_foreign\n",
    "def debuger(test_sample_text,database,sql):\n",
    "  instruction = \"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to fix the given SQLite SQL QUERY for any issues. If there are any problems, fix them. If there are no issues, return the SQLite SQL QUERY as is.\n",
    "#### Use the following instructions for fixing the SQL QUERY:\n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement when using SUM, AVG, MAX, MIN and COUNT.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\n",
    "\n",
    "\"\"\"\n",
    "  fields = find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  fields += \"Primary_keys = \" + find_primary_keys_MYSQL_like(database)\n",
    "  prompt = instruction + fields+ '#### Question: ' + test_sample_text + '\\n#### SQLite SQL QUERY\\n' + sql +'\\n#### SQLite FIXED SQL QUERY\\nSELECT'\n",
    "  return prompt\n",
    "def GPT4_generation(prompt):\n",
    "  for data in chatbot.ask(prompt, conversation_id=config['conversation_id'], parent_id=None):\n",
    "    response = data[\"message\"]\n",
    "  GPT4_clear_conversations(chatbot)\n",
    "  return response\n",
    "\n",
    "def GPT4_debug(prompt):\n",
    "  for data in chatbot.ask(prompt, conversation_id=config['conversation_id'], parent_id=None):\n",
    "    response = data[\"message\"]\n",
    "  GPT4_clear_conversations(chatbot)\n",
    "  return response\n",
    "\n",
    "def GPT4_clear_conversations(chatbot):\n",
    "  chatbot.clear_conversations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spider_schema,spider_primary,spider_foreign = creatiing_schema(DATASET_SCHEMA)\n",
    "val_df = load_data(DATASET)\n",
    "SQL_list = load_sql_txt(BEFORE_SQL)\n",
    "crt_time = time.strftime(\"%m-%d-%H:%M:%S\", time.localtime())\n",
    "print(f\"Number of data samples {val_df.shape[0]}\")\n",
    "start_index = 451\n",
    "end_index = 524\n",
    "with open(f'fix_debug-{start_index}-{end_index}-{crt_time}.log', 'w') as record, open(f'fix_debug_record-{start_index}-{end_index}-{crt_time}.log', 'w') as schema_link:\n",
    "    CODEX = []\n",
    "    for ((index, row), SQL) in zip(val_df.iterrows(), SQL_list):\n",
    "        if index < start_index: continue #for testing\n",
    "        print(\"index:\", index)\n",
    "        # if index < 10: continue #for testing\n",
    "        record.write(f\"\\nindex is {index}\"+ '\\n')\n",
    "        record.write(row['query']+ '\\n')\n",
    "        record.write(row['question']+ '\\n')\n",
    "        \n",
    "        record.write('SQL generation:'+ '\\n')\n",
    "        record.write(SQL+ '\\n')\n",
    "        debugged_SQL = None\n",
    "        while debugged_SQL is None:\n",
    "            try:\n",
    "                with open('tmp.txt', 'w') as f:\n",
    "                    f.write(debuger(row['question'], row['db_id'], SQL))\n",
    "                debugged_SQL = GPT4_debug(debuger(row['question'], row['db_id'], SQL)).replace(\"\\n\", \" \")\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                pass\n",
    "        SQL = \"SELECT \" + debugged_SQL\n",
    "        record.write('self correction:'+ '\\n')\n",
    "        record.write(SQL+ '\\n')\n",
    "        schema_link.write(SQL + '\\n')\n",
    "        CODEX.append([row['question'], SQL, row['query'], row['db_id']])\n",
    "        #break\n",
    "        # if index == 30: break\n",
    "        if index == end_index: break\n",
    "# df = pd.DataFrame(CODEX, columns=['NLQ', 'PREDICTED SQL', 'GOLD SQL', 'DATABASE'])\n",
    "# results = df['PREDICTED SQL'].tolist()\n",
    "# with open(OUTPUT_FILE, 'w') as f:\n",
    "#     for line in results:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file_name = 'my_result/origin/2/new_classification/code_tag_classification_record'\n",
    "with open(f'{record_file_name}.log', 'r') as record:\n",
    "    record_line = [line.strip() for line in record]\n",
    "    \n",
    "# cmp_result_file_name = '/home/r10525068/ensemble/result/gpt-3.5/din_sql/origin/2/din_sql_before_cmp_result.txt'\n",
    "# with open(f'{cmp_result_file_name}', 'r') as record:\n",
    "#     result_line = [line.strip() for line in record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{record_file_name}_cmp_result_before.txt', 'w') as cmp_result:\n",
    "    for index, line in enumerate(result_line):\n",
    "        if 'Correct:' in line:\n",
    "            cmp_result.write(line+'\\n')\n",
    "            # cmp_result.write(line.split('Correct: ')[1]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{record_file_name}_cmp_result_before.txt', 'r') as cmp_result:\n",
    "    cmp_result_list = [line.strip() for line in cmp_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cmp_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_links\n",
    "with open(f'{record_file_name}_schema_link.txt', 'w') as schema_link:\n",
    "    crt_index = -1\n",
    "    for index, line in enumerate(record_line):\n",
    "        # if 'index is' in line:\n",
    "        #     crt_index = int(record_line[index].split('index is ')[1])\n",
    "        #     schema_link.write(f'index: {crt_index}\\n')\n",
    "        #     schema_link.write(f'Gold SQL: {record_line[index+1]}\\n')\n",
    "        if 'schema_links:' in line:\n",
    "            # schema_link.write(f'schema_links: {record_line[index+1]}\\n')\n",
    "            schema_link.write(f'{record_line[index+1]}\\n')\n",
    "            # schema_link.write(f'{cmp_result_list[crt_index]}\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL生成到自我修正\n",
    "with open(f'{record_file_name}_get_before_after.txt', 'w') as schema_link,  open(f'{record_file_name}_get_before.txt', 'w') as before,  open(f'{record_file_name}_get_after.txt', 'w') as after:\n",
    "    for index, line in enumerate(record_line):\n",
    "        if 'index is' in line:\n",
    "            schema_link.write(record_line[index] + '\\n')\n",
    "        if 'self correction:' in line:\n",
    "            schema_link.write('before:\\n')\n",
    "            schema_link.write(record_line[index-1] + '\\n')\n",
    "            before.write(record_line[index-1] + '\\n')\n",
    "            schema_link.write('after:\\n')\n",
    "            schema_link.write(record_line[index+1] + '\\n\\n')\n",
    "            after.write(record_line[index+1] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類結果\n",
    "nested_str = ['INTERSECT', 'UNION', 'EXCEPT', 'IN', 'NOT IN']\n",
    "with open(f'{record_file_name}_check_classifiaction.txt', 'w') as f:\n",
    "    crt_index = -1\n",
    "    results = []\n",
    "    result = {}\n",
    "    for index, line in enumerate(record_line):\n",
    "        if 'index is' in line:\n",
    "            if result:\n",
    "                result['pred_label'] = ''\n",
    "                f.write(f'Pred Label: {result[\"pred_label\"]}\\n')\n",
    "                result['correct'] = (result['gold_label'] == result['pred_label'])\n",
    "                f.write(f'Correct: {result[\"correct\"]}\\n\\n')\n",
    "                results.append(result)\n",
    "                result = {}\n",
    "            crt_index = record_line[index]\n",
    "            result['index'] = crt_index\n",
    "            f.write(record_line[index] + '\\n')\n",
    "            gold_sql = record_line[index+1]\n",
    "            f.write(f'Gold SQL: {gold_sql}\\n')\n",
    "            result['gold'] = gold_sql\n",
    "            gold_sql_list = gold_sql.lower().split(' ')\n",
    "            if gold_sql_list.count('select') > 1 or gold_sql_list.count('(select'):\n",
    "                result['gold_label'] = 'nested'\n",
    "            elif 'join' in gold_sql.lower():\n",
    "                result['gold_label'] = 'non-nested'\n",
    "            else:\n",
    "                result['gold_label'] = 'easy'\n",
    "            f.write(f'Gold Label: {result[\"gold_label\"]}\\n')\n",
    "        if 'Label:' in line and result:\n",
    "            try:\n",
    "                result['pred_label'] = line.split('\"')[1].lower().strip()\n",
    "            except:\n",
    "                result['pred_label'] = line.split('Label:')[1].lower().strip()\n",
    "            f.write(f'Pred Label: {result[\"pred_label\"]}\\n')\n",
    "            result['correct'] = (result['gold_label'] == result['pred_label'])\n",
    "            f.write(f'Correct: {result[\"correct\"]}\\n\\n')\n",
    "            results.append(result)\n",
    "            result = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只分成 有無 JOIN 的分類結果\n",
    "with open(f'{record_file_name}_check_two_classifiaction.txt', 'w') as f:\n",
    "    crt_index = -1\n",
    "    results = []\n",
    "    result = {}\n",
    "    for index, line in enumerate(record_line):\n",
    "        if 'index is' in line:\n",
    "            if result:\n",
    "                result['pred_label'] = ''\n",
    "                f.write(f'Pred Label: {result[\"pred_label\"]}\\n')\n",
    "                result['correct'] = (result['gold_label'] == result['pred_label'])\n",
    "                f.write(f'Correct: {result[\"correct\"]}\\n\\n')\n",
    "                results.append(result)\n",
    "                result = {}\n",
    "            crt_index = record_line[index]\n",
    "            result['index'] = crt_index\n",
    "            f.write(record_line[index] + '\\n')\n",
    "            gold_sql = record_line[index+1]\n",
    "            f.write(f'Gold SQL: {gold_sql}\\n')\n",
    "            result['gold'] = gold_sql\n",
    "            gold_sql_list = gold_sql.lower().split(' ')\n",
    "            if gold_sql_list.count('join') or gold_sql_list.count('(select'):\n",
    "                result['gold_label'] = 'complex'\n",
    "            else:\n",
    "                result['gold_label'] = 'easy'\n",
    "            f.write(f'Gold Label: {result[\"gold_label\"]}\\n')\n",
    "        if 'Label:' in line and result:\n",
    "            try:\n",
    "                result['pred_label'] = line.split('\"')[1].lower().strip()\n",
    "            except:\n",
    "                result['pred_label'] = line.split('Label:')[1].lower().strip()\n",
    "            f.write(f'Pred Label: {result[\"pred_label\"]}\\n')\n",
    "            result['correct'] = (result['gold_label'] == result['pred_label'])\n",
    "            f.write(f'Correct: {result[\"correct\"]}\\n\\n')\n",
    "            results.append(result)\n",
    "            result = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_to_easy: 426\n",
      "easy_to_non: 50\n",
      "easy_to_nested: 60\n",
      "easy_to_: 8\n",
      "non_to_easy: 74\n",
      "non_to_non: 188\n",
      "non_to_nested: 63\n",
      "non_to_: 6\n",
      "nested_to_easy: 18\n",
      "nested_to_non: 31\n",
      "nested_to_nested: 110\n",
      "nested_to_: 0\n"
     ]
    }
   ],
   "source": [
    "label = {'easy': 0, 'non-nested': 1, 'nested': 2, '': 3}\n",
    "# gold_to_pred\n",
    "analysis = [\n",
    "    {'label': 'easy_to_easy', 'record': []},\n",
    "    {'label': 'easy_to_non', 'record': []},\n",
    "    {'label': 'easy_to_nested', 'record': []},\n",
    "    {'label': 'easy_to_', 'record': []},\n",
    "    {'label': 'non_to_easy', 'record': []},\n",
    "    {'label': 'non_to_non', 'record': []},\n",
    "    {'label': 'non_to_nested', 'record': []},\n",
    "    {'label': 'non_to_', 'record': []},\n",
    "    {'label': 'nested_to_easy', 'record': []},\n",
    "    {'label': 'nested_to_non', 'record': []},\n",
    "    {'label': 'nested_to_nested', 'record': []},\n",
    "    {'label': 'nested_to_', 'record': []},\n",
    "]\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    # if i < 1020: continue\n",
    "    try:\n",
    "        idx = label[result['gold_label']]*4 + label[result['pred_label']]\n",
    "    except:\n",
    "        idx = label[result['gold_label']]*4 + label['']\n",
    "    # print(result['gold_label'], result['pred_label'])\n",
    "    # print(idx)\n",
    "    # print()\n",
    "    analysis[idx]['record'].append(result)\n",
    "    # if i > 349: break\n",
    "\n",
    "# with open(f'{record_file_name}_get_classification_analysis.txt', 'w') as f:\n",
    "for ana in analysis:\n",
    "    print(ana['label'] + ': ' + str(len(ana['record'])))\n",
    "        # f.write(ana['label'] + ': ' + str(len(ana['record'])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{record_file_name}_three_classification.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result['pred_label']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_result/origin/2/new_classification/short_question_classification_record'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinsql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
