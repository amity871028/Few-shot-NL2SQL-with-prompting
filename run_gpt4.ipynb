{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revChatGPT.V1 import Chatbot\n",
    "config = {\n",
    "    \"access_token_3744\": \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ik1UaEVOVUpHTkVNMVFURTRNMEZCTWpkQ05UZzVNRFUxUlRVd1FVSkRNRU13UmtGRVFrRXpSZyJ9.eyJodHRwczovL2FwaS5vcGVuYWkuY29tL3Byb2ZpbGUiOnsiZW1haWwiOiJhbWl0eTM3NDQwMzlAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWV9LCJodHRwczovL2FwaS5vcGVuYWkuY29tL2F1dGgiOnsidXNlcl9pZCI6InVzZXItMHRkNFF6T2N4WFl2QWo1NkJmVGdENUxjIn0sImlzcyI6Imh0dHBzOi8vYXV0aDAub3BlbmFpLmNvbS8iLCJzdWIiOiJnb29nbGUtb2F1dGgyfDEwMTI2MDUzNzY2MDA0NzY3OTY5MyIsImF1ZCI6WyJodHRwczovL2FwaS5vcGVuYWkuY29tL3YxIiwiaHR0cHM6Ly9vcGVuYWkub3BlbmFpLmF1dGgwYXBwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2ODYwMjc3NDAsImV4cCI6MTY4NzIzNzM0MCwiYXpwIjoiVGRKSWNiZTE2V29USHROOTVueXl3aDVFNHlPbzZJdEciLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIG1vZGVsLnJlYWQgbW9kZWwucmVxdWVzdCBvcmdhbml6YXRpb24ucmVhZCBvcmdhbml6YXRpb24ud3JpdGUifQ.mOmJYnC9ES_o4qa07JAEAJ9JsYQE3Xqj6ZfuEdobMECBLvm3EzqvZlaMBi0l04NdzvwA_t04Tn1oylVIKhjBkqf8DMdmktT4qxXOba6fWBHRim5B99d7p_8G_STK_1HmX42BHP8C1vxBfwfq5qFb4l1KNpxnHhExfW91-GjiZ-vdqEi33nhQ4QtrmYVDtXDLClfVueyb3Cmsvi2_8_HUPMykf14uCjrneLPukDSFmtszIsFktc0avTINgk8VCdCvTR3k7e7Qe52FePJ-LpC56xitkyL5j1WZ0Qa6xWqcwQ0u1rTFaNNCA6YK60Nj19pJnsEqVnMgIKWtqYsda4KjRg\",\n",
    "    \"access_token_vic\": \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ik1UaEVOVUpHTkVNMVFURTRNMEZCTWpkQ05UZzVNRFUxUlRVd1FVSkRNRU13UmtGRVFrRXpSZyJ9.eyJodHRwczovL2FwaS5vcGVuYWkuY29tL3Byb2ZpbGUiOnsiZW1haWwiOiJ2aWM4MjQxN0BnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZX0sImh0dHBzOi8vYXBpLm9wZW5haS5jb20vYXV0aCI6eyJ1c2VyX2lkIjoidXNlci1BeFluR1R0WFdzS2xpalVZNGlXMGdBWDkifSwiaXNzIjoiaHR0cHM6Ly9hdXRoMC5vcGVuYWkuY29tLyIsInN1YiI6ImF1dGgwfDY0Nzc2Njc1NTIwZTgyZDlmZDU3NDczNCIsImF1ZCI6WyJodHRwczovL2FwaS5vcGVuYWkuY29tL3YxIiwiaHR0cHM6Ly9vcGVuYWkub3BlbmFpLmF1dGgwYXBwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2ODYwMjc1NDksImV4cCI6MTY4NzIzNzE0OSwiYXpwIjoiVGRKSWNiZTE2V29USHROOTVueXl3aDVFNHlPbzZJdEciLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIG1vZGVsLnJlYWQgbW9kZWwucmVxdWVzdCBvcmdhbml6YXRpb24ucmVhZCBvcmdhbml6YXRpb24ud3JpdGUifQ.TVrcf05bt1IlRZwy2PrsmiRefSfysmSchsp94TifjYilDYgqS1ENXiA_Wa3ZDkxP0CYgX5WGC0j_wLltMbHfqMUMyqbVV0OkVDRfwCaj8XRE-m-Lso0gTwUzCHmkvlrQLdTcxGAVCejtZyV_68uKAV7oTyWAM5hiuH7Us8TJcVhyTysZYKk_oWSlT6Vi2EWA0nVdGS-1lCGd9WV1PSkB5lOdFURDoAvLvVXQVZ_CoG38_CQUZyFrvTf0FkUMdT-uYbgt6n9OE3To-5vYoEPlVtoFbF2irVgvkdbdjAlZFDdEkoT9hv77Z55QZs9xo_zr2FMOV5mEEdKIY8Jmnf08pg\",\n",
    "    \"access_token_lab\": \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ik1UaEVOVUpHTkVNMVFURTRNMEZCTWpkQ05UZzVNRFUxUlRVd1FVSkRNRU13UmtGRVFrRXpSZyJ9.eyJodHRwczovL2FwaS5vcGVuYWkuY29tL3Byb2ZpbGUiOnsiZW1haWwiOiJja2xhYi5udHVAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWV9LCJodHRwczovL2FwaS5vcGVuYWkuY29tL2F1dGgiOnsidXNlcl9pZCI6InVzZXItUU1RejRNcm5MbHYwcUxUVXJ6NE5tM0VYIn0sImlzcyI6Imh0dHBzOi8vYXV0aDAub3BlbmFpLmNvbS8iLCJzdWIiOiJhdXRoMHw2NDYzMjFiNjY4MDMyYmYyODg4MzBlYzMiLCJhdWQiOlsiaHR0cHM6Ly9hcGkub3BlbmFpLmNvbS92MSIsImh0dHBzOi8vb3BlbmFpLm9wZW5haS5hdXRoMGFwcC5jb20vdXNlcmluZm8iXSwiaWF0IjoxNjg1NDU4MjQ3LCJleHAiOjE2ODY2Njc4NDcsImF6cCI6IlRkSkljYmUxNldvVEh0Tjk1bnl5d2g1RTR5T282SXRHIiwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSBlbWFpbCBtb2RlbC5yZWFkIG1vZGVsLnJlcXVlc3Qgb3JnYW5pemF0aW9uLnJlYWQgb3JnYW5pemF0aW9uLndyaXRlIn0.cABY08HZ0G9RRsfxT49ktOVoWVDyvWT3wVkW4ho6L8pRhymeCagyFMyIf2d583tiv1r4oD2FWqyKbgDALAx5cVj_L4-_pIWrxZfC0KNqsJwPdYEV0QO6uxEcPsmRZX7b8IYFjSSNo6OsAX8X1fsVW4LxSIybZ5TE6xrMdSkFH6Ib4vPkKTSXIUeWB9O021daN8eBaekZk1slfRTyZ7ew5iHST-x466GdUvV0ZsHmaqcU9_UFpDG3KCLLTSR4juEtao7YKeZnwRNhsLGe_5Obe-lveRnx3DR88m1ehCBLwPH-Kzs4lqiVmjxXzTOjIY1gQoNp3InuxtGdowuArwsB6w\",\n",
    "    \"access_token_charlie\": \"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ik1UaEVOVUpHTkVNMVFURTRNMEZCTWpkQ05UZzVNRFUxUlRVd1FVSkRNRU13UmtGRVFrRXpSZyJ9.eyJodHRwczovL2FwaS5vcGVuYWkuY29tL3Byb2ZpbGUiOnsiZW1haWwiOiJjaGFybGllc21haWwyMDIyMTAzMEBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZX0sImh0dHBzOi8vYXBpLm9wZW5haS5jb20vYXV0aCI6eyJ1c2VyX2lkIjoidXNlci1RdDJZcmxoOEdYWmZnRDg1bVpGc0VPS1YifSwiaXNzIjoiaHR0cHM6Ly9hdXRoMC5vcGVuYWkuY29tLyIsInN1YiI6Imdvb2dsZS1vYXV0aDJ8MTE1MTk4MTc4NDQ5ODY3MTc4MzY1IiwiYXVkIjpbImh0dHBzOi8vYXBpLm9wZW5haS5jb20vdjEiLCJodHRwczovL29wZW5haS5vcGVuYWkuYXV0aDBhcHAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTY4NTkzMDUyMCwiZXhwIjoxNjg3MTQwMTIwLCJhenAiOiJUZEpJY2JlMTZXb1RIdE45NW55eXdoNUU0eU9vNkl0RyIsInNjb3BlIjoib3BlbmlkIHByb2ZpbGUgZW1haWwgbW9kZWwucmVhZCBtb2RlbC5yZXF1ZXN0IG9yZ2FuaXphdGlvbi5yZWFkIG9yZ2FuaXphdGlvbi53cml0ZSJ9.T6BrKyLnQ1PnwpFI4Of5JnikJ1vgivsn7tF-PJUb2OMjmdaFNpMxouImCUGsEChGxOG0jEOpvEB6Bia-xYx5AuqlUsTKnHDvCLWhurzCzoRmfyKUa9wTNSD-uHLCyqny0UrjO-3mN29xY_zlgbAqUpHNZvy8HES2UzUCL-tVkYhC8Q03kEEGbSaiM-gemimt3HeQBdvtNcisrM6ckwJDd2i-SRo7hXGrYJ0RRFJmJPUIO_Mdkclnm8id8DZVYOSJf7SNMB8STdmGTbO0cAQZp15MHDrreeONX7HOOWiky0hOHAWdz0ZZeNvA6rhcd2rnru6BRglB1OHEj1qWPxiQ9A\",\n",
    "    \"conversation_id\": None,#\"6c261dc4-2d5b-4e15-8d01-2c94d7db27f5\"\n",
    "    'model': 'gpt-4'\n",
    "    # 'model': 'text-davinci-002-render-sha'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pay_chatbot(index):\n",
    "    # print('pay index: ', index)\n",
    "    pay_count = 4\n",
    "    name = \"\"\n",
    "    if index % pay_count == 0:\n",
    "        name = \"access_token_3744\"\n",
    "    elif index % pay_count == 1:\n",
    "        name = \"access_token_vic\"\n",
    "    elif index % pay_count == 2:\n",
    "        name = \"access_token_lab\"\n",
    "    elif index % pay_count == 3:\n",
    "        name = \"access_token_charlie\"\n",
    "\n",
    "    print(f\"pay name: {name}, index: {index}\", end=\"\\r\", flush=True)\n",
    "    chatbot = Chatbot(config={\"access_token\": config[name]})\n",
    "    return chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_config = {\n",
    "    \"dateset\": \"./data/\",\n",
    "    \"output\": \"predicted_sql.txt\"\n",
    "}\n",
    "# if sys.argv[1] == \"--dataset\" and sys.argv[3] == \"--output\":\n",
    "DATASET_SCHEMA = tmp_config['dateset']+\"tables.json\"\n",
    "DATASET = tmp_config['dateset']+\"dev.json\"\n",
    "OUTPUT_FILE = tmp_config['output']\n",
    "FILE_PATH = 'my_result/origin/2/resqsql/2'\n",
    "BEFORE_SQL = f\"{FILE_PATH}/resd_record_get_before.txt\"\n",
    "# else:\n",
    "#     raise Exception(\"Please use this format python CoT.py --dataset data/ --output predicted_sql.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(DATASET):\n",
    "  return pd.read_json(DATASET)\n",
    "\n",
    "def load_sql_txt(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    return [line.strip() for line in f]\n",
    "\n",
    "def find_foreign_keys_MYSQL_like(db_name):\n",
    "  df = spider_foreign[spider_foreign['Database name'] == db_name]\n",
    "  output = \"[\"\n",
    "  for index, row in df.iterrows():\n",
    "    output += row['First Table Name'] + '.' + row['First Table Foreign Key'] + \" = \" + row['Second Table Name'] + '.' + row['Second Table Foreign Key'] + ','\n",
    "  output= output[:-1] + \"]\"\n",
    "  return output\n",
    "def find_fields_MYSQL_like(db_name):\n",
    "  df = spider_schema[spider_schema['Database name'] == db_name]\n",
    "  df = df.groupby(' Table Name')\n",
    "  output = \"\"\n",
    "  for name, group in df:\n",
    "    output += \"Table \" +name+ ', columns = ['\n",
    "    for index, row in group.iterrows():\n",
    "      output += row[\" Field Name\"]+','\n",
    "    output = output[:-1]\n",
    "    output += \"]\\n\"\n",
    "  return output\n",
    "def find_primary_keys_MYSQL_like(db_name):\n",
    "  df = spider_primary[spider_primary['Database name'] == db_name]\n",
    "  output = \"[\"\n",
    "  for index, row in df.iterrows():\n",
    "    output += row['Table Name'] + '.' + row['Primary Key'] +','\n",
    "  output = output[:-1]\n",
    "  output += \"]\\n\"\n",
    "  return output\n",
    "def creatiing_schema(DATASET_JSON):\n",
    "    schema_df = pd.read_json(DATASET_JSON)\n",
    "    schema_df = schema_df.drop(['column_names','table_names'], axis=1)\n",
    "    schema = []\n",
    "    f_keys = []\n",
    "    p_keys = []\n",
    "    for index, row in schema_df.iterrows():\n",
    "        tables = row['table_names_original']\n",
    "        col_names = row['column_names_original']\n",
    "        col_types = row['column_types']\n",
    "        foreign_keys = row['foreign_keys']\n",
    "        primary_keys = row['primary_keys']\n",
    "        for col, col_type in zip(col_names, col_types):\n",
    "            index, col_name = col\n",
    "            if index == -1:\n",
    "                for table in tables:\n",
    "                    schema.append([row['db_id'], table, '*', 'text'])\n",
    "            else:\n",
    "                schema.append([row['db_id'], tables[index], col_name, col_type])\n",
    "        for primary_key in primary_keys:\n",
    "            index, column = col_names[primary_key]\n",
    "            p_keys.append([row['db_id'], tables[index], column])\n",
    "        for foreign_key in foreign_keys:\n",
    "            first, second = foreign_key\n",
    "            first_index, first_column = col_names[first]\n",
    "            second_index, second_column = col_names[second]\n",
    "            f_keys.append([row['db_id'], tables[first_index], tables[second_index], first_column, second_column])\n",
    "    spider_schema = pd.DataFrame(schema, columns=['Database name', ' Table Name', ' Field Name', ' Type'])\n",
    "    spider_primary = pd.DataFrame(p_keys, columns=['Database name', 'Table Name', 'Primary Key'])\n",
    "    spider_foreign = pd.DataFrame(f_keys,\n",
    "                        columns=['Database name', 'First Table Name', 'Second Table Name', 'First Table Foreign Key',\n",
    "                                 'Second Table Foreign Key'])\n",
    "    return spider_schema,spider_primary,spider_foreign\n",
    "def debuger_new(test_sample_text,database,sql):\n",
    "  instruction = \"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to fix the given SQLite SQL QUERY for any issues. If there are any problems, fix them. If there are no issues, return the SQLite SQL QUERY as is.\n",
    "#### Use the following instructions for fixing the SQL QUERY:\n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement when using SUM, AVG, MAX, MIN and COUNT.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "  fields = find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  fields += \"Primary_keys = \" + find_primary_keys_MYSQL_like(database)\n",
    "  prompt = instruction + fields+ '#### Question: ' + test_sample_text + '\\n#### SQLite SQL QUERY\\n' + sql +'\\n#### SQLite FIXED SQL QUERY\\nSELECT'\n",
    "  return prompt\n",
    "def debuger(test_sample_text,database,sql):\n",
    "  instruction = \"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to fix the given SQLite SQL QUERY for any issues. If there are any problems, fix them. If there are no issues, return the SQLite SQL QUERY as is.\n",
    "#### Use the following instructions for fixing the SQL QUERY:\n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "  fields = find_fields_MYSQL_like(database)\n",
    "  fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + '\\n'\n",
    "  fields += \"Primary_keys = \" + find_primary_keys_MYSQL_like(database)\n",
    "  prompt = instruction + fields+ '#### Question: ' + test_sample_text + '\\n#### SQLite SQL QUERY\\n' + sql +'\\n#### SQLite FIXED SQL QUERY\\nSELECT'\n",
    "  return prompt\n",
    "\n",
    "def GPT4_debug(chatbot, prompt):\n",
    "  for data in chatbot.ask(prompt, conversation_id=config['conversation_id'], parent_id=None, model=config['model']):\n",
    "    response = data[\"message\"]\n",
    "  GPT4_clear_conversations(chatbot)\n",
    "  return response\n",
    "\n",
    "def GPT4_clear_conversations(chatbot):\n",
    "  chatbot.clear_conversations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154, 164, 192, 203, 205, 290, 306, 334, 398, 429, 445, 512, 560, 568, 580, 622, 685, 728, 733, 807, 812, 837, 866, 987, 1002]\n",
      "[16, 34, 82, 138, 209, 263, 266, 270, 299, 323, 336, 340, 354, 419, 468, 531, 553, 724, 840, 849, 858, 860, 957, 973, 1017]\n",
      "[53, 64, 83, 103, 256, 257, 346, 396, 410, 422, 423, 460, 595, 635, 676, 759, 773, 786, 787, 810, 814, 899, 900, 910, 919]\n",
      "[130, 166, 232, 277, 312, 344, 373, 452, 503, 529, 537, 542, 548, 637, 712, 743, 754, 755, 760, 785, 906, 907, 916, 941, 954]\n"
     ]
    }
   ],
   "source": [
    "with open('hardness.txt', 'r') as h:\n",
    "    hard_list = [line.strip() for line in h]\n",
    "    \n",
    "easy_indices = [i for i, x in enumerate(hard_list) if x == 'easy']\n",
    "medium_indices = [i for i, x in enumerate(hard_list) if x == 'medium']\n",
    "hard_indices = [i for i, x in enumerate(hard_list) if x == 'hard']\n",
    "extra_indices = [i for i, x in enumerate(hard_list) if x == 'extra']\n",
    "\n",
    "import random\n",
    "count = 25#50\n",
    "selected_easy_indices = [154, 164, 192, 203, 205, 290, 306, 334, 398, 429, 445, 512, 560, 568, 580, 622, 685, 728, 733, 807, 812, 837, 866, 987, 1002]\n",
    "selected_medium_indices = [16, 34, 82, 138, 209, 263, 266, 270, 299, 323, 336, 340, 354, 419, 468, 531, 553, 724, 840, 849, 858, 860, 957, 973, 1017]\n",
    "selected_hard_indices = [53, 64, 83, 103, 256, 257, 346, 396, 410, 422, 423, 460, 595, 635, 676, 759, 773, 786, 787, 810, 814, 899, 900, 910, 919]\n",
    "selected_extra_indices = [130, 166, 232, 277, 312, 344, 373, 452, 503, 529, 537, 542, 548, 637, 712, 743, 754, 755, 760, 785, 906, 907, 916, 941, 954]\n",
    "\n",
    "# selected_easy_indices = sorted(random.sample(easy_indices, count))\n",
    "# selected_medium_indices = sorted(random.sample(medium_indices, count))\n",
    "# selected_hard_indices = sorted(random.sample(hard_indices, count))\n",
    "# selected_extra_indices = sorted(random.sample(extra_indices, count))\n",
    "print(selected_easy_indices)\n",
    "print(selected_medium_indices)\n",
    "print(selected_hard_indices)\n",
    "print(selected_extra_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_easy_indices2=[45, 46, 181, 188, 206, 292, 314, 352, 430, 432, 437, 510, 511, 519, 559, 567, 578, 585, 586, 587, 614, 734, 829, 968, 993]\n",
    "selected_medium_indices2=[11, 21, 77, 90, 112, 121, 140, 242, 302, 337, 392, 480, 481, 490, 498, 608, 634, 690, 771, 800, 821, 892, 952, 1010, 1032]\n",
    "selected_hard_indices2=[28, 32, 37, 115, 159, 282, 293, 332, 402, 447, 463, 475, 499, 544, 596, 606, 684, 854, 857, 894, 895, 926, 975, 1015, 1028]\n",
    "selected_extra_indices2=[25, 60, 99, 107, 131, 177, 225, 228, 229, 238, 343, 574, 576, 697, 747, 764, 767, 775, 777, 887, 917, 929, 930, 938, 955]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_numbers = selected_easy_indices+selected_easy_indices2\n",
    "excluded_numbers = sorted(excluded_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92, 144, 145, 187, 199, 260, 378, 384, 441, 477, 589, 600, 653, 657, 659, 677, 719, 822, 824, 826, 834, 871, 881, 969, 1007]\n",
      "[15, 33, 48, 68, 94, 245, 251, 253, 353, 386, 394, 405, 408, 435, 506, 727, 738, 770, 816, 850, 893, 956, 963, 1018, 1023]\n",
      "[37, 84, 104, 106, 115, 142, 255, 281, 285, 286, 293, 345, 404, 463, 465, 571, 606, 753, 778, 855, 856, 897, 898, 926, 1028]\n",
      "[41, 59, 86, 97, 158, 165, 168, 172, 176, 221, 239, 284, 427, 539, 541, 561, 740, 756, 765, 774, 784, 795, 818, 819, 845]\n"
     ]
    }
   ],
   "source": [
    "excluded_numbers = selected_easy_indices + selected_easy_indices2\n",
    "selected_easy_indices3 = sorted(random.sample([num for num in easy_indices if num not in excluded_numbers], count))\n",
    "excluded_numbers = selected_medium_indices + selected_medium_indices2\n",
    "selected_medium_indices3 = sorted(random.sample([num for num in medium_indices if num not in excluded_numbers], count))\n",
    "excluded_numbers = selected_hard_indices + selected_medium_indices2\n",
    "selected_hard_indices3 = sorted(random.sample([num for num in hard_indices if num not in excluded_numbers], count))\n",
    "excluded_numbers = selected_extra_indices + selected_extra_indices2\n",
    "selected_extra_indices3 = sorted(random.sample([num for num in extra_indices if num not in excluded_numbers], count))\n",
    "print(selected_easy_indices3)\n",
    "print(selected_medium_indices3)\n",
    "print(selected_hard_indices3)\n",
    "print(selected_extra_indices3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples 1034\n",
      "index: 11, level: medium\n",
      "pay name: access_token_3744, index: 0\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debugged_SQL: The provided SQLite SQL query seems to be correct in this context as it does adhere to the rules that you've stated. It is counting the number of singers from each country, which is exactly what the question is asking. It also has an appropriate GROUP BY clause on the 'country' column. The fixed query remains the same:  ```sql SELECT country , count(*) FROM singer GROUP BY country ```\n",
      "index: 21, level: medium\n",
      "debugged_SQL: count(DISTINCT concert_ID) FROM concert WHERE Year = 2014 OR Year = 2015\n",
      "index: 25, level: extra\n",
      "debugged_SQL: stadium.Name, stadium.Capacity  FROM stadium  JOIN concert ON stadium.Stadium_ID = concert.Stadium_ID WHERE concert.Year > 2013 GROUP BY stadium.Stadium_ID, stadium.Name, stadium.Capacity ORDER BY COUNT(concert.concert_ID) DESC LIMIT 1;\n",
      "index: 28, level: hard\n",
      "debugged_SQL: Name FROM stadium WHERE Stadium_ID NOT IN (SELECT Stadium_ID FROM concert)\n",
      "index: 32, level: hard\n",
      "debugged_SQL: Name FROM stadium WHERE Stadium_ID NOT IN (SELECT Stadium_ID FROM concert WHERE Year = 2014)\n",
      "index: 37, level: hard\n",
      "debugged_SQL: DISTINCT singer.Name FROM singer_in_concert JOIN concert ON singer_in_concert.concert_ID = concert.concert_ID JOIN singer ON singer_in_concert.Singer_ID = singer.Singer_ID WHERE concert.Year = 2014\n",
      "index: 45, level: easy\n",
      "debugged_SQL: count(*) FROM Pets WHERE weight > 10\n",
      "index: 46, level: easy\n",
      "debugged_SQL: COUNT(*) FROM Pets WHERE weight > 10\n",
      "index: 60, level: extra\n",
      "debugged_SQL: The provided SQLite SQL query has a problem in the WHERE clause. It selects the students who have either a cat or a dog. However, the question asks for the students who have both a cat and a dog. This problem requires a change in the logic to ensure it only returns students who own both a cat and a dog. This change means two separate queries will have to be made, one for students with cats and one for students with dogs. These two queries will then be intersected to only return the students who are in both result sets.   Below is the corrected SQLite SQL query:  ```sql SELECT DISTINCT fname FROM Student JOIN Has_Pet ON Student.StuID = Has_Pet.StuID JOIN Pets ON Has_Pet.PetID = Pets.PetID WHERE Pets.PetType = 'cat'  INTERSECT  SELECT DISTINCT fname FROM Student JOIN Has_Pet ON Student.StuID = Has_Pet.StuID JOIN Pets ON Has_Pet.PetID = Pets.PetID WHERE Pets.PetType = 'dog' ```  This query will first select the students with cats and then separately select the students with dogs. It will then return only the students that are in both lists, fulfilling the requirement of the question.\n",
      "index: 77, level: medium\n",
      "debugged_SQL: Has_Pet.PetID FROM Student JOIN Has_Pet ON Student.StuID = Has_Pet.StuID WHERE Student.LName = 'Smith'\n",
      "index: 90, level: medium\n",
      "debugged_SQL: continents.ContId, continents.Continent, COUNT(countries.CountryName)  FROM continents  JOIN countries ON continents.ContId = countries.Continent  GROUP BY continents.ContId, continents.Continent\n",
      "index: 99, level: extra\n",
      "debugged_SQL: DISTINCT Maker  FROM car_makers  JOIN model_list ON car_makers.Id = model_list.Maker  JOIN car_names ON model_list.Model = car_names.Model  JOIN cars_data ON car_names.MakeId = cars_data.Id  WHERE cars_data.Year = 1970\n",
      "index: 107, level: extra\n",
      "debugged_SQL: countries.CountryName, COUNT(car_makers.Id) AS NumberOfCarMakers FROM countries  JOIN car_makers  ON countries.CountryId = car_makers.Country GROUP BY countries.CountryName  ORDER BY NumberOfCarMakers DESC  LIMIT 1;\n",
      "index: 112, level: medium\n",
      "debugged_SQL: Accelerate FROM cars_data  JOIN car_names ON cars_data.Id = car_names.MakeId  WHERE car_names.Model = 'amc hornet sportabout (sw)'\n",
      "index: 115, level: hard\n",
      "debugged_SQL: count(DISTINCT model_list.model) FROM countries  JOIN car_makers ON countries.CountryId = car_makers.Country  JOIN model_list ON car_makers.Id = model_list.Maker  WHERE countries.CountryName = 'USA'\n",
      "index: 121, level: medium\n",
      "debugged_SQL: Maker, Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker JOIN car_names ON model_list.Model = car_names.Model\n",
      "index: 131, level: extra\n",
      "debugged_SQL: The make of the car models and the maximum horsepower they have can be fetched from the `cars_data` and `car_names` tables. These two tables can be joined on the `id` of `cars_data` and `makeid` of `car_names` which is given in the foreign keys list. However, the `cylinders` column is likely in the `cars_data` table, but it's not explicitly specified. Thus, it's assumed here.  The provided query seems correct as per the information provided. There's only one minor issue: SQL is case-insensitive, but it's common practice to write SQL keywords in uppercase for better readability. Here is the improved version:  ```sql SELECT MAX(Horsepower), Make  FROM cars_data  JOIN car_names ON cars_data.Id = car_names.MakeId  WHERE Cylinders = 3  GROUP BY Make; ```  This query will return the maximum horsepower and the make of the car models for those cars which have 3 cylinders. Please make sure the column names match exactly with your database schema.\n",
      "index: 140, level: medium\n",
      "debugged_SQL: The original SQL query is correct as per the question. It correctly selects the maximum acceleration for all different cylinders from the 'cars_data' table and groups the result by cylinders. Therefore, the fixed SQLite SQL Query would be the same as the original:  ```sql SELECT cylinders, MAX(accelerate) FROM cars_data GROUP BY cylinders ``` The other tables and keys provided are not needed for this particular query as all the necessary data is contained within the 'cars_data' table.\n",
      "index: 159, level: hard\n",
      "debugged_SQL: count(*)  FROM cars_data  WHERE Accelerate > (SELECT MAX(Horsepower) FROM cars_data)\n",
      "index: 177, level: extra\n",
      "debugged_SQL: countries.countryid, countries.countryname  FROM countries  JOIN car_makers ON car_makers.country = countries.countryid  LEFT JOIN model_list ON car_makers.id = model_list.maker  LEFT JOIN car_names ON model_list.model = car_names.model  GROUP BY countries.countryid HAVING COUNT(DISTINCT car_makers.id) > 3 OR \"fiat\" IN (     SELECT DISTINCT car_names.make      FROM car_names      JOIN model_list ON car_names.model = model_list.model     WHERE model_list.maker = car_makers.id )\n",
      "index: 181, level: easy\n",
      "debugged_SQL: Abbreviation FROM airlines WHERE Airline = \"JetBlue Airways\"\n",
      "index: 188, level: easy\n",
      "debugged_SQL: count(*) FROM airlines2\n",
      "index: 206, level: easy\n",
      "debugged_SQL: count(*) FROM flights WHERE DestAirport = 'ATO'\n",
      "index: 225, level: extra\n",
      "debugged_SQL: DestAirport FROM flights GROUP BY DestAirport ORDER BY COUNT(*) DESC LIMIT 1\n",
      "index: 228, level: extra\n",
      "debugged_SQL: DestAirport FROM flights GROUP BY DestAirport ORDER BY count(*) ASC LIMIT 1\n",
      "index: 229, level: extra\n",
      "debugged_SQL: Airline FROM flights GROUP BY Airline ORDER BY COUNT(*) DESC LIMIT 1\n",
      "index: 238, level: extra\n",
      "debugged_SQL: Airline FROM airlines WHERE uid IN (     SELECT Airline FROM flights WHERE (SourceAirport = 'APG' AND DestAirport <> 'APG') OR (SourceAirport = 'CVO' AND DestAirport <> 'CVO') ) GROUP BY Airline HAVING COUNT(DISTINCT SourceAirport) = 2\n",
      "index: 242, level: medium\n",
      "debugged_SQL: airlines.Airline FROM flights JOIN airlines ON flights.Airline = airlines.uid GROUP BY airlines.Airline HAVING COUNT(flights.FlightNo) >= 10\n",
      "index: 282, level: hard\n",
      "debugged_SQL: Name FROM employee WHERE Employee_ID NOT IN (SELECT Employee_ID FROM evaluation)\n",
      "index: 292, level: easy\n",
      "debugged_SQL: * FROM hiring index: 30\n",
      "index: 293, level: hard\n",
      "debugged_SQL: district FROM shop WHERE district IN (     SELECT district FROM shop WHERE number_products < 3000 ) AND district IN (     SELECT district FROM shop WHERE number_products > 10000 ) GROUP BY district\n",
      "index: 302, level: medium\n",
      "debugged_SQL: Document_Name, Template_ID FROM Documents WHERE Document_Description LIKE '%w%'\n",
      "index: 314, level: easy\n",
      "debugged_SQL: Documents.Template_ID FROM Documents GROUP BY Documents.Template_ID HAVING COUNT(DISTINCT Documents.Document_ID) > 1\n",
      "index: 332, level: hard\n",
      "debugged_SQL: Template_Type_Code  FROM Templates  GROUP BY Template_Type_Code  ORDER BY COUNT(*) DESC  LIMIT 1\n",
      "index: 337, level: medium\n",
      "debugged_SQL: Templates.Template_Type_Code FROM Templates JOIN Documents ON Templates.Template_ID = Documents.Template_ID WHERE Documents.Document_Name = \"Data base\"\n",
      "index: 343, level: extra\n",
      "debugged_SQL: T.Template_Type_Code FROM Templates T INNER JOIN Documents D ON T.Template_ID = D.Template_ID GROUP BY T.Template_Type_Code ORDER BY COUNT(D.Document_ID) DESC LIMIT 1\n",
      "index: 352, level: easy\n",
      "debugged_SQL: Template_Type_Code FROM Ref_Template_Types WHERE Template_Type_Description = \"Book\"\n",
      "index: 392, level: medium\n",
      "debugged_SQL: Hometown FROM teacher ORDER BY age ASC LIMIT 1\n",
      "index: 402, level: hard\n",
      "debugged_SQL: teacher.Name, course.Course  FROM teacher  JOIN course_arrange ON teacher.Teacher_ID = course_arrange.Teacher_ID  JOIN course ON course.Course_ID = course_arrange.Course_ID  ORDER BY teacher.Name ASC\n",
      "index: 430, level: easy\n",
      "debugged_SQL: count(*) FROM players 40\n",
      "index: 432, level: easy\n",
      "debugged_SQL: count(*) FROM matches41\n",
      "index: 437, level: easy\n",
      "debugged_SQL: avg(winner_rank) FROM matches\n",
      "index: 447, level: hard\n",
      "debugged_SQL: DISTINCT p.first_name, p.last_name  FROM matches m JOIN players p ON m.winner_id = p.player_id WHERE m.year = 2013 AND p.player_id IN (     SELECT p.player_id      FROM matches m     JOIN players p ON m.winner_id = p.player_id     WHERE m.year = 2016 )\n",
      "index: 463, level: hard\n",
      "debugged_SQL: winner_name, winner_rank_points  FROM matches  GROUP BY winner_id  ORDER BY COUNT(*) DESC  LIMIT 1\n",
      "index: 475, level: hard\n",
      "debugged_SQL: country_code, COUNT(player_id) AS NumberOfPlayers FROM players GROUP BY country_code ORDER BY NumberOfPlayers DESC LIMIT 1\n",
      "index: 480, level: medium\n",
      "debugged_SQL: ranking_date ,  SUM(tours)  FROM rankings  GROUP BY ranking_date\n",
      "index: 481, level: medium\n",
      "debugged_SQL: year , count(*) FROM matches GROUP BY year\n",
      "index: 490, level: medium\n",
      "debugged_SQL: hand ,  COUNT(*) FROM players GROUP BY hand\n",
      "index: 498, level: medium\n",
      "debugged_SQL: DISTINCT battle.id ,  battle.name FROM battle JOIN ship ON battle.id  =  ship.lost_in_battle WHERE ship.ship_type  =  'Brig'\n",
      "index: 499, level: hard\n",
      "debugged_SQL: battle.id, battle.name  FROM battle  JOIN ship ON battle.id = ship.lost_in_battle  JOIN death ON ship.id = death.caused_by_ship_id  GROUP BY battle.id  HAVING sum(death.killed) > 10\n",
      "index: 510, level: easy\n",
      "debugged_SQL: count(*) FROM Coursesex: 51\n",
      "index: 511, level: easy\n",
      "debugged_SQL: course_description FROM Courses WHERE course_name = 'math'\n",
      "index: 519, level: easy\n",
      "debugged_SQL: COUNT(DISTINCT degree_summary_name) FROM Degree_Programs\n",
      "index: 544, level: hard\n",
      "debugged_SQL: semester_name FROM Semesters WHERE NOT EXISTS (SELECT semester_id FROM Student_Enrolment WHERE Semesters.semester_id = Student_Enrolment.semester_id)\n",
      "index: 559, level: easy\n",
      "debugged_SQL: first_name FROM Students WHERE permanent_address_id != current_address_id\n",
      "index: 567, level: easy\n",
      "debugged_SQL: The SQL query seems correct and it aligns with the question asked. So, the SQL query remains the same:  ```sql SELECT count(transcript_id) FROM Transcripts ```  This query is used to count the number of transcripts released. It counts the total number of `transcript_id` present in the `Transcripts` table.\n",
      "index: 574, level: extra\n",
      "debugged_SQL: The given SQLite SQL query seems correct for the question provided. There seems to be no need for corrections. The query joins two tables `Transcripts` and `Transcript_Contents` using the foreign key relationship between `transcript_id` from both tables, and groups the data by `transcript_id`. It then counts the number of rows in each group, orders them in ascending order, and finally selects the `transcript_date` and `transcript_id` for the transcript with the least number of rows.  The query is reproduced as is:  ```sql SELECT transcript_date, transcript_id  FROM Transcripts AS T1  JOIN Transcript_Contents AS T2 ON T1.transcript_id  =  T2.transcript_id  GROUP BY T1.transcript_id  ORDER BY COUNT(*) ASC  LIMIT 1; ``` This query follows all the mentioned instructions correctly. The join operation is performed using the correct foreign keys, the `ORDER BY` clause is used to order the number of results, the `GROUP BY` clause is used to group the results by `transcript_id` and `LIMIT 1` is used to get the result with the least number of rows. There is no need for `DESC` or `DISTINCT` in this case.\n",
      "index: 576, level: extra\n",
      "debugged_SQL: semester_id  FROM Student_Enrolment  WHERE degree_program_id IN (     SELECT degree_program_id      FROM Degree_Programs      WHERE degree_summary_name IN ('Masters', 'Bachelors'))  GROUP BY semester_id  HAVING COUNT(DISTINCT degree_program_id) > 1 \n",
      "index: 578, level: easy\n",
      "debugged_SQL: The provided SQL query appears to be correct. The query correctly uses subqueries to get distinct address IDs from both the 'current_address_id' and 'permanent_address_id' columns of the 'students' table. It then selects distinct records from the 'addresses' table where the 'address_id' matches those extracted from the subqueries. So, there's no need to make any changes to the SQL query.  Therefore, the fixed SQL query would remain the same:  ```sql SELECT DISTINCT line_1, line_2, line_3, city, zip_postcode, state_province_county, country, other_address_details FROM addresses WHERE address_id IN (SELECT DISTINCT current_address_id FROM students) OR address_id IN (SELECT DISTINCT permanent_address_id FROM students) ```\n",
      "index: 585, level: easy\n",
      "debugged_SQL: Title FROM Cartoon ORDER BY Title\n",
      "index: 586, level: easy\n",
      "debugged_SQL: Title FROM Cartoon ORDER BY Title\n",
      "index: 587, level: easy\n",
      "debugged_SQL: Title FROM Cartoon WHERE Directed_by = \"Ben Jones\"\n",
      "index: 596, level: hard\n",
      "debugged_SQL: Country ,  COUNT(*)  FROM TV_Channel  GROUP BY Country  ORDER BY COUNT(*) DESC  LIMIT 1\n",
      "index: 606, level: hard\n",
      "debugged_SQL: Language, COUNT(*) AS Number_of_Channels  FROM TV_Channel  GROUP BY Language  ORDER BY Number_of_Channels ASC  LIMIT 1\n",
      "index: 608, level: medium\n",
      "debugged_SQL: Language, COUNT(*) FROM TV_Channel GROUP BY Language\n",
      "index: 614, level: easy\n",
      "debugged_SQL: Episode FROM TV_series ORDER BY Rating\n",
      "index: 634, level: medium\n",
      "debugged_SQL: DISTINCT TV_Channel.Country  FROM TV_Channel  JOIN Cartoon ON TV_Channel.id = Cartoon.Channel  WHERE Cartoon.Written_by = 'Todd Casey'\n",
      "index: 684, level: hard\n",
      "debugged_SQL: Name FROM people WHERE People_ID NOT IN (SELECT People_ID FROM poker_player)\n",
      "index: 690, level: medium\n",
      "debugged_SQL: MAX(area_code), MIN(area_code) FROM AREA_CODE_STATE\n",
      "index: 697, level: extra\n",
      "debugged_SQL: COUNT(*) FROM CONTESTANTS WHERE contestant_number NOT IN (SELECT DISTINCT contestant_number FROM VOTES)\n",
      "index: 734, level: easy\n",
      "debugged_SQL: count(DISTINCT GovernmentForm) FROM country WHERE Continent = 'Africa'\n",
      "index: 747, level: extra\n",
      "debugged_SQL: Name FROM country WHERE Code IN (SELECT CountryCode FROM countrylanguage WHERE Language = 'English') AND Code IN (SELECT CountryCode FROM countrylanguage WHERE Language = 'French')\n",
      "index: 764, level: extra\n",
      "debugged_SQL: AVG(LifeExpectancy)  FROM country  WHERE Code NOT IN (     SELECT CountryCode      FROM countrylanguage      WHERE Language = 'English'      AND IsOfficial = 'T' )\n",
      "index: 767, level: extra\n",
      "debugged_SQL: SUM(country.Population)  FROM country  WHERE country.Code NOT IN (     SELECT countrylanguage.CountryCode      FROM countrylanguage      WHERE countrylanguage.Language = 'English' )\n",
      "index: 771, level: medium\n",
      "debugged_SQL: count(DISTINCT countrylanguage.Language)  FROM country  JOIN countrylanguage ON country.Code = countrylanguage.CountryCode  WHERE country.IndepYear < 1930 AND countrylanguage.IsOfficial = 'T'\n",
      "index: 775, level: extra\n",
      "debugged_SQL: A.Name  FROM country A  WHERE A.Continent = 'Africa' AND A.Population < (     SELECT MIN(B.Population)      FROM country B      WHERE B.Continent = 'Asia' )\n",
      "index: 777, level: extra\n",
      "debugged_SQL: DISTINCT Name FROM country WHERE Continent = 'Asia' AND Population > (SELECT MAX(Population) FROM country WHERE Continent = 'Africa')\n",
      "index: 800, level: medium\n",
      "debugged_SQL: Name , SurfaceArea FROM country ORDER BY SurfaceArea DESC LIMIT 5\n",
      "index: 821, level: medium\n",
      "debugged_SQL: CountryCode FROM countrylanguage WHERE Language = 'Spanish' AND Percentage > 50\n",
      "index: 829, level: easy\n",
      "debugged_SQL: Record_Company FROM orchestra ORDER BY Year_of_Founded DESC\n",
      "index: 854, level: hard\n",
      "debugged_SQL: Orchestra FROM orchestra WHERE Orchestra_ID NOT IN (SELECT Orchestra_ID FROM performance)\n",
      "index: 857, level: hard\n",
      "debugged_SQL: record_company FROM orchestra WHERE year_of_founded  <  2003 OR year_of_founded  >  2003 GROUP BY record_company\n",
      "index: 887, level: extra\n",
      "debugged_SQL: T1.name  FROM Highschooler AS T1  JOIN Friend AS T2 ON T1.ID  =  T2.student_id  GROUP BY T1.name  ORDER BY count(T2.friend_id) DESC  LIMIT 1\n",
      "index: 892, level: medium\n",
      "debugged_SQL: count(Friend.friend_id)  FROM Highschooler  JOIN Friend ON Highschooler.ID = Friend.student_id  WHERE Highschooler.name = 'Kyle'\n",
      "index: 894, level: hard\n",
      "debugged_SQL: The SQL query you provided seems correct according to the information provided. It selects all the student ids from the highschooler table where the same id does not exist in the friend table, implying that those students do not have any friends.  So, the fixed SQL query would be the same as your initial SQL query:  ```sql SELECT id FROM highschooler WHERE NOT EXISTS (SELECT * FROM friend WHERE highschooler.id = friend.student_id) ```  This will return the ids of all students who do not have any friends.\n",
      "index: 895, level: hard\n",
      "debugged_SQL: ID FROM Highschooler WHERE ID NOT IN (SELECT student_id FROM Friend)\n",
      "index: 917, level: extra\n",
      "debugged_SQL: min(grade) FROM Highschooler WHERE ID NOT IN (SELECT DISTINCT student_id FROM Friend)\n",
      "index: 926, level: hard\n",
      "debugged_SQL: first_name  FROM Professionals  WHERE first_name NOT IN (SELECT name FROM Dogs)  UNION  SELECT first_name  FROM Owners  WHERE first_name NOT IN (SELECT name FROM Dogs)\n",
      "index: 929, level: extra\n",
      "debugged_SQL: The provided SQL query appears to be correct. It selects the 'professional_id', 'role_code', and 'email_address' from the Professionals table where there doesn't exist a corresponding record in the Treatments table for the given professional_id. This aligns with the question's requirements. Therefore, the SQL query is returned as is:  ```sql SELECT professional_id , role_code , email_address FROM Professionals WHERE NOT EXISTS (SELECT * FROM Treatments WHERE Treatments.professional_id  =  Professionals.professional_id) ```\n",
      "index: 930, level: extra\n",
      "debugged_SQL: Dogs.owner_id, Owners.first_name, Owners.last_name  FROM Dogs  INNER JOIN Owners ON Dogs.owner_id = Owners.owner_id  GROUP BY Dogs.owner_id  ORDER BY count(*) DESC  LIMIT 1\n",
      "index: 938, level: extra\n",
      "debugged_SQL: treatment_type_description  FROM Treatment_Types  JOIN Treatments ON Treatment_Types.treatment_type_code = Treatments.treatment_type_code  GROUP BY treatment_type_description  ORDER BY SUM(Treatments.cost_of_treatment) ASC  LIMIT 1\n",
      "index: 952, level: medium\n",
      "debugged_SQL: Owners.first_name , Dogs.name FROM Owners INNER JOIN Dogs ON Owners.owner_id  =  Dogs.owner_id\n",
      "index: 955, level: extra\n",
      "debugged_SQL: Dogs.name, Treatments.date_of_treatment FROM Dogs  JOIN Breeds ON Dogs.breed_code = Breeds.breed_code  JOIN Treatments ON Dogs.dog_id = Treatments.dog_id WHERE Breeds.breed_code IN (     SELECT breed_code      FROM Dogs      GROUP BY breed_code      HAVING COUNT(*) = (         SELECT COUNT(*)          FROM Dogs          GROUP BY breed_code          ORDER BY COUNT(*) ASC          LIMIT 1     ) ) ORDER BY Dogs.name, Treatments.date_of_treatment;\n",
      "index: 968, level: easy\n",
      "debugged_SQL: COUNT(DISTINCT Professionals.professional_id)  FROM Professionals JOIN Treatments ON Professionals.professional_id = Treatments.professional_id WHERE Treatments.professional_id IS NOT NULL\n",
      "index: 975, level: hard\n",
      "debugged_SQL: count(*) FROM Dogs WHERE age  <  (SELECT avg(age) FROM Dogs)\n",
      "index: 993, level: easy\n",
      "debugged_SQL: The SQL query you provided seems correct according to the given question. There are no JOIN operations needed as all necessary data is present in the 'Charges' table. We just need to select the highest charge amount, which is achieved by sorting the charge_amount in descending order and then limiting the output to 1.   Thus, the query remains the same:  ```sql SELECT charge_amount FROM Charges ORDER BY charge_amount DESC LIMIT 1 ```\n",
      "index: 1010, level: medium\n",
      "debugged_SQL: name FROM singer ORDER BY net_worth_millions DESC LIMIT 1\n",
      "index: 1015, level: hard\n",
      "debugged_SQL: Citizenship, COUNT(*) as count FROM singer GROUP BY Citizenship ORDER BY count DESC LIMIT 1\n",
      "index: 1028, level: hard\n",
      "debugged_SQL: DISTINCT citizenship FROM singer WHERE birth_year < 1945 OR birth_year > 1955\n",
      "index: 1032, level: medium\n",
      "pay name: access_token_vic, index: 9410939\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:660\u001b[0m, in \u001b[0;36mChatbot.__check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 660\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    661\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://ai.fakeopen.com/api/conversation",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     debugged_SQL \u001b[39m=\u001b[39m GPT4_debug(\n\u001b[1;32m     27\u001b[0m             get_pay_chatbot(pay_index),\n\u001b[1;32m     28\u001b[0m         debuger(row[\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m], row[\u001b[39m\"\u001b[39;49m\u001b[39mdb_id\u001b[39;49m\u001b[39m\"\u001b[39;49m], SQL),\n\u001b[1;32m     29\u001b[0m     )\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     pay_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 105\u001b[0m, in \u001b[0;36mGPT4_debug\u001b[0;34m(chatbot, prompt)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGPT4_debug\u001b[39m(chatbot, prompt):\n\u001b[0;32m--> 105\u001b[0m   \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m chatbot\u001b[39m.\u001b[39mask(prompt, conversation_id\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mconversation_id\u001b[39m\u001b[39m'\u001b[39m], parent_id\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, model\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    106\u001b[0m     response \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:548\u001b[0m, in \u001b[0;36mChatbot.ask\u001b[0;34m(self, prompt, conversation_id, parent_id, model, auto_continue, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m messages \u001b[39m=\u001b[39m [\n\u001b[1;32m    540\u001b[0m     {\n\u001b[1;32m    541\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     },\n\u001b[1;32m    546\u001b[0m ]\n\u001b[0;32m--> 548\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_messages(\n\u001b[1;32m    549\u001b[0m     messages,\n\u001b[1;32m    550\u001b[0m     conversation_id\u001b[39m=\u001b[39mconversation_id,\n\u001b[1;32m    551\u001b[0m     parent_id\u001b[39m=\u001b[39mparent_id,\n\u001b[1;32m    552\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    553\u001b[0m     auto_continue\u001b[39m=\u001b[39mauto_continue,\n\u001b[1;32m    554\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    555\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:502\u001b[0m, in \u001b[0;36mChatbot.post_messages\u001b[0;34m(self, messages, conversation_id, parent_id, model, auto_continue, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m    489\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maction\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    490\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m     ),\n\u001b[1;32m    500\u001b[0m }\n\u001b[0;32m--> 502\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__send_request(\n\u001b[1;32m    503\u001b[0m     data,\n\u001b[1;32m    504\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    505\u001b[0m     auto_continue\u001b[39m=\u001b[39mauto_continue,\n\u001b[1;32m    506\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:357\u001b[0m, in \u001b[0;36mChatbot.__send_request\u001b[0;34m(self, data, auto_continue, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    352\u001b[0m     url\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url\u001b[39m}\u001b[39;00m\u001b[39mconversation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    353\u001b[0m     data\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mdumps(data),\n\u001b[1;32m    354\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    355\u001b[0m     stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    356\u001b[0m )\n\u001b[0;32m--> 357\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__check_response(response)\n\u001b[1;32m    359\u001b[0m finish_details \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:66\u001b[0m, in \u001b[0;36mlogger.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 66\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     67\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/envs/test_revchatgpt/lib/python3.11/site-packages/revChatGPT/V1.py:667\u001b[0m, in \u001b[0;36mChatbot.__check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    662\u001b[0m error \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mError(\n\u001b[1;32m    663\u001b[0m     source\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOpenAI\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    664\u001b[0m     message\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mtext,\n\u001b[1;32m    665\u001b[0m     code\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code,\n\u001b[1;32m    666\u001b[0m )\n\u001b[0;32m--> 667\u001b[0m \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n",
      "\u001b[0;31mError\u001b[0m: OpenAI: {\"detail\":{\"message\":\"You have sent too many messages to the model. Please try again later.\",\"code\":\"model_cap_exceeded\",\"clears_in\":4959}} (code: 429)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         pay_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 35\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n\u001b[1;32m     36\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     37\u001b[0m SQL \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSELECT \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m debugged_SQL\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "spider_schema,spider_primary,spider_foreign = creatiing_schema(DATASET_SCHEMA)\n",
    "val_df = load_data(DATASET)\n",
    "SQL_list = load_sql_txt(BEFORE_SQL)\n",
    "crt_time = time.strftime(\"%m-%d-%H:%M:%S\", time.localtime())\n",
    "print(f\"Number of data samples {val_df.shape[0]}\")\n",
    "start_index = 0\n",
    "end_index = 1034\n",
    "pay_index = 0\n",
    "dd = 0\n",
    "with open(f'{FILE_PATH}/gpt4_correction_record-{start_index}-{end_index}-{crt_time}.log', 'w') as record, open(f'{FILE_PATH}/gpt4_correction_pred_sql-{start_index}-{end_index}-{crt_time}.log', 'w') as pred:\n",
    "    CODEX = []\n",
    "    for ((index, row), SQL) in zip(val_df.iterrows(), SQL_list):\n",
    "        if index < start_index: continue\n",
    "        if index not in selected_easy_indices2 and index not in selected_medium_indices2 and index not in selected_hard_indices2 and index not in selected_extra_indices2:\n",
    "            continue\n",
    "        print(f\"index: {index}, level: {hard_list[index]}\")\n",
    "        record.write(f\"\\nindex is {index}, level is {hard_list[index]}\"+ '\\n')\n",
    "        record.write(row['query']+ '\\n')\n",
    "        record.write(row['question']+ '\\n')\n",
    "        \n",
    "        record.write('SQL generation:'+ '\\n')\n",
    "        record.write(SQL+ '\\n')\n",
    "        debugged_SQL = None\n",
    "        while debugged_SQL is None:\n",
    "            try:\n",
    "                debugged_SQL = GPT4_debug(\n",
    "                        get_pay_chatbot(pay_index),\n",
    "                    debuger(row[\"question\"], row[\"db_id\"], SQL),\n",
    "                ).replace(\"\\n\", \" \")\n",
    "                pay_index += 1\n",
    "                print(\"debugged_SQL:\", debugged_SQL)\n",
    "                dd += 1\n",
    "            except:\n",
    "                pay_index += 1\n",
    "                time.sleep(3)\n",
    "                pass\n",
    "        SQL = \"SELECT \" + debugged_SQL\n",
    "        record.write('self correction:'+ '\\n')\n",
    "        record.write(SQL+ '\\n')\n",
    "        pred.write(SQL + '\\n')\n",
    "        CODEX.append([row['question'], SQL, row['query'], row['db_id']])\n",
    "        #break\n",
    "        # if index == 30: break\n",
    "        if index == end_index: break\n",
    "        if dd == 100: break\n",
    "# df = pd.DataFrame(CODEX, columns=['NLQ', 'PREDICTED SQL', 'GOLD SQL', 'DATABASE'])\n",
    "# results = df['PREDICTED SQL'].tolist()\n",
    "# with open(OUTPUT_FILE, 'w') as f:\n",
    "#     for line in results:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'sql_norm_del_value_record'\n",
    "with open(f'{file_name}.log', 'r') as record:\n",
    "    record_line = [line.strip() for line in record]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "with open(f'{file_name}_get_before_after.txt', 'w') as before_after,  open(f'{file_name}_get_before.txt', 'w') as before,  open(f'{file_name}_get_after.txt', 'w') as after:\n",
    "    for index, line in enumerate(record_line):\n",
    "        if 'index is' in line:\n",
    "            before_after.write(record_line[index] + '\\n')\n",
    "        if 'self correction:' in line:\n",
    "            before_after.write('before:\\n')\n",
    "            before_after.write(record_line[index-1] + '\\n')\n",
    "            before.write(record_line[index-1] + '\\n')\n",
    "            before_after.write('after:\\n')\n",
    "            before_after.write(record_line[index+1] + '\\n')\n",
    "            after.write(record_line[index+1] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "nested_str = ['INTERSECT', 'UNION', 'EXCEPT', 'IN', 'NOT IN']\n",
    "with open(f'{file_name}_check_schema_link.txt', 'w') as f:\n",
    "    crt_index = -1\n",
    "    results = []\n",
    "    result = {}\n",
    "    for index, line in enumerate(record_line):\n",
    "        if 'index is' in line:\n",
    "            crt_index = record_line[index]\n",
    "            result['index'] = crt_index\n",
    "            f.write(record_line[index] + '\\n')\n",
    "            gold_sql = record_line[index+1]\n",
    "            f.write(f'Gold SQL: {gold_sql}\\n')\n",
    "            result['gold'] = gold_sql\n",
    "            gold_sql_list = gold_sql.lower().split(' ')\n",
    "            if gold_sql_list.count('select') > 1 or gold_sql_list.count('(select'):\n",
    "                result['gold_label'] = 'nested'\n",
    "            elif 'join' in gold_sql.lower():\n",
    "                result['gold_label'] = 'non-nested'\n",
    "            else:\n",
    "                result['gold_label'] = 'easy'\n",
    "            f.write(f'Gold Label: {result[\"gold_label\"]}\\n')\n",
    "        if 'Label:' in line and result:\n",
    "            result['pred_label'] = line.split('\"')[1].lower()\n",
    "            f.write(f'Pred Label: {result[\"pred_label\"]}\\n')\n",
    "            result['correct'] = (result['gold_label'] == result['pred_label'])\n",
    "            f.write(f'Correct: {result[\"correct\"]}\\n\\n')\n",
    "            results.append(result)\n",
    "            result = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hardness \n",
    "with open('dev_hardness_info.txt', 'r') as f:\n",
    "    info_line = [line.strip() for line in f]\n",
    "\n",
    "with open('hardness.txt', 'w') as h:\n",
    "    for line in info_line:\n",
    "        if 'Level: ' in line:\n",
    "            h.write(f'{line.split(\": \")[1]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinsql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
