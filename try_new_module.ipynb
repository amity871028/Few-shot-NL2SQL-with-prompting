{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revChatGPT.V1 import Chatbot\n",
    "import random\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "f = open(\"config.json\")\n",
    "config = json.load(f)\n",
    "\n",
    "\n",
    "def get_free_chatbot(index):\n",
    "    # print('free index: ', index)\n",
    "    free_count = 3\n",
    "    name = \"\"\n",
    "    if index % free_count == 0:\n",
    "        name = \"access_token_r105\"\n",
    "    elif index % free_count == 1:\n",
    "        name = \"access_token_vic_yy\"\n",
    "    elif index % free_count == 2:\n",
    "        name = \"access_token_tony\"\n",
    "    print(f\"free name: {name}, index: {index}\", end=\"\\r\", flush=True)\n",
    "    chatbot = Chatbot(config={\"access_token\": config[name]})\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "def get_pay_chatbot(index):\n",
    "    # print('pay index: ', index)\n",
    "    pay_count = 4\n",
    "    name = \"\"\n",
    "    if index % pay_count == 0:\n",
    "        name = \"access_token_vic\"\n",
    "    elif index % pay_count == 1:\n",
    "        name = \"access_token_vic\"\n",
    "    elif index % pay_count == 2:\n",
    "        name = \"access_token_lab\"\n",
    "    elif index % pay_count == 3:\n",
    "        rand_int = random.randrange(5)\n",
    "        # 為了讓這隻帳號不要被跑太多次\n",
    "        if rand_int == 0:\n",
    "            name = \"access_token_lab\"\n",
    "        elif rand_int == 1:\n",
    "            name = \"access_token_lab\"\n",
    "        elif rand_int == 2:\n",
    "            name = \"access_token_vic\"\n",
    "        elif rand_int == 3:\n",
    "            name = \"access_token_charlie\"\n",
    "\n",
    "    print(f\"pay name: {name}, index: {index}\", end=\"\\r\", flush=True)\n",
    "    chatbot = Chatbot(config={\"access_token\": config[name]})\n",
    "    return chatbot\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "tmp_config = {\"dateset\": \"./data/\", \"output\": \"predicted_sql.txt\"}\n",
    "# if sys.argv[1] == \"--dataset\" and sys.argv[3] == \"--output\":\n",
    "DATASET_SCHEMA = tmp_config[\"dateset\"] + \"tables.json\"\n",
    "DATASET = tmp_config[\"dateset\"] + \"dev.json\"\n",
    "OUTPUT_FILE = tmp_config[\"output\"]\n",
    "FILE_PATH = \"my_result/resqsql/3\"\n",
    "BEFORE_SQL = f\"{FILE_PATH}/resd_predicted_sql.txt\"\n",
    "SCHEMA_LINK_FILE = \"my_result/resqsql/schema_link.txt\"\n",
    "DB_PATH = '/home/r10525068/ensemble/data/database'\n",
    "# else:\n",
    "#     raise Exception(\"Please use this format python CoT.py --dataset data/ --output predicted_sql.txt\")\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_non_english(string):\n",
    "    pattern = r\"^[^a-zA-Z]+|[^a-zA-Z]+$\"\n",
    "    cleaned_string = re.sub(pattern, \"\", string)\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "def load_data(DATASET):\n",
    "    return pd.read_json(DATASET)\n",
    "\n",
    "\n",
    "def load_sql_txt(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.strip() for line in f]\n",
    "\n",
    "\n",
    "def find_foreign_keys_MYSQL_like(db_name):\n",
    "    df = spider_foreign[spider_foreign[\"Database name\"] == db_name]\n",
    "    output = \"[\"\n",
    "    for index, row in df.iterrows():\n",
    "        output += (\n",
    "            row[\"First Table Name\"]\n",
    "            + \".\"\n",
    "            + row[\"First Table Foreign Key\"]\n",
    "            + \" = \"\n",
    "            + row[\"Second Table Name\"]\n",
    "            + \".\"\n",
    "            + row[\"Second Table Foreign Key\"]\n",
    "            + \",\"\n",
    "        )\n",
    "    output = output[:-1] + \"]\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def find_fields_MYSQL_like(db_name):\n",
    "    df = spider_schema[spider_schema[\"Database name\"] == db_name]\n",
    "    df = df.groupby(\" Table Name\")\n",
    "    output = \"\"\n",
    "    for name, group in df:\n",
    "        output += \"Table \" + name + \", columns = [\"\n",
    "        for index, row in group.iterrows():\n",
    "            output += row[\" Field Name\"] + \",\"\n",
    "        output = output[:-1]\n",
    "        output += \"]\\n\"\n",
    "    return output\n",
    "\n",
    "def find_cols_with_entities_by_sql(SQL):\n",
    "    sql_str_list = SQL.split(' ')\n",
    "    tb = None\n",
    "    col = None\n",
    "    cols_have_entities = []\n",
    "    for sql_idx, str in enumerate(sql_str_list):\n",
    "        if str.startswith('\\''):\n",
    "            try:\n",
    "                tb_col = sql_str_list[sql_idx-2]\n",
    "                if '.' in tb_col:\n",
    "                    col = remove_non_english(tb_col.split('.')[1])\n",
    "                    cols_have_entities.append(col)\n",
    "                else:\n",
    "                    col = remove_non_english(tb_col.strip(' '))\n",
    "                    cols_have_entities.append(col)\n",
    "            except:\n",
    "                print('cannot split it:', sql_str_list[sql_idx-2])\n",
    "    return cols_have_entities\n",
    "\n",
    "\n",
    "def find_table_by_fields(db_name, cols_have_entities):\n",
    "    df = spider_schema[spider_schema[\"Database name\"] == db_name]\n",
    "    df = df.groupby(\" Table Name\")\n",
    "    tables = {}\n",
    "    for name, group in df:\n",
    "        for index, row in group.iterrows():\n",
    "            if row[\" Field Name\"] in cols_have_entities:\n",
    "                col = row[\" Field Name\"]\n",
    "                if name not in tables:\n",
    "                    tables[name] = []\n",
    "                tables[name].append(col)\n",
    "    return tables\n",
    "\n",
    "def find_entities_by_table_fields(db, tables_with_cols):\n",
    "    connection = sqlite3.connect(f'{DB_PATH}/{db}/{db}.sqlite')\n",
    "    cursor = connection.cursor()\n",
    "    cols_with_entities = {}\n",
    "    for table in tables_with_cols.keys():\n",
    "        for col in tables_with_cols[table]:\n",
    "            res = cursor.execute(f\"SELECT DISTINCT {col} FROM {table} LIMIT 10\")\n",
    "            value = res.fetchall()\n",
    "            try:\n",
    "                int(value[0][0])\n",
    "            except:\n",
    "                cols_with_entities[col] = [v[0] for v  in value]\n",
    "    return cols_with_entities\n",
    "    \n",
    "\n",
    "def find_primary_keys_MYSQL_like(db_name):\n",
    "    df = spider_primary[spider_primary[\"Database name\"] == db_name]\n",
    "    output = \"[\"\n",
    "    for index, row in df.iterrows():\n",
    "        output += row[\"Table Name\"] + \".\" + row[\"Primary Key\"] + \",\"\n",
    "    output = output[:-1]\n",
    "    output += \"]\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def creatiing_schema(DATASET_JSON):\n",
    "    schema_df = pd.read_json(DATASET_JSON)\n",
    "    schema_df = schema_df.drop([\"column_names\", \"table_names\"], axis=1)\n",
    "    schema = []\n",
    "    f_keys = []\n",
    "    p_keys = []\n",
    "    for index, row in schema_df.iterrows():\n",
    "        tables = row[\"table_names_original\"]\n",
    "        col_names = row[\"column_names_original\"]\n",
    "        col_types = row[\"column_types\"]\n",
    "        foreign_keys = row[\"foreign_keys\"]\n",
    "        primary_keys = row[\"primary_keys\"]\n",
    "        for col, col_type in zip(col_names, col_types):\n",
    "            index, col_name = col\n",
    "            if index == -1:\n",
    "                for table in tables:\n",
    "                    schema.append([row[\"db_id\"], table, \"*\", \"text\"])\n",
    "            else:\n",
    "                schema.append([row[\"db_id\"], tables[index], col_name, col_type])\n",
    "        for primary_key in primary_keys:\n",
    "            index, column = col_names[primary_key]\n",
    "            p_keys.append([row[\"db_id\"], tables[index], column])\n",
    "        for foreign_key in foreign_keys:\n",
    "            first, second = foreign_key\n",
    "            first_index, first_column = col_names[first]\n",
    "            second_index, second_column = col_names[second]\n",
    "            f_keys.append(\n",
    "                [\n",
    "                    row[\"db_id\"],\n",
    "                    tables[first_index],\n",
    "                    tables[second_index],\n",
    "                    first_column,\n",
    "                    second_column,\n",
    "                ]\n",
    "            )\n",
    "    spider_schema = pd.DataFrame(\n",
    "        schema, columns=[\"Database name\", \" Table Name\", \" Field Name\", \" Type\"]\n",
    "    )\n",
    "    spider_primary = pd.DataFrame(\n",
    "        p_keys, columns=[\"Database name\", \"Table Name\", \"Primary Key\"]\n",
    "    )\n",
    "    spider_foreign = pd.DataFrame(\n",
    "        f_keys,\n",
    "        columns=[\n",
    "            \"Database name\",\n",
    "            \"First Table Name\",\n",
    "            \"Second Table Name\",\n",
    "            \"First Table Foreign Key\",\n",
    "            \"Second Table Foreign Key\",\n",
    "        ],\n",
    "    )\n",
    "    return spider_schema, spider_primary, spider_foreign\n",
    "\n",
    "def entities_debuger(test_sample_text, database, sql, cols_have_entities):\n",
    "    instruction = \"\"\"#### For the given question, use the provided cell values to fix the given SQLite SQL QUERY for any issues. If there are any problems, fix them. If there are no issues, return the SQLite SQL QUERY as is.\n",
    "\"\"\"\n",
    "    tables_with_cols = find_table_by_fields(database, cols_have_entities)\n",
    "    cols_with_entities = find_entities_by_table_fields(database, tables_with_cols)\n",
    "    print(cols_with_entities)\n",
    "    fields = \"\"\n",
    "    for col in cols_with_entities.keys():\n",
    "        fields += f\"column = [{col}\\n\"\n",
    "        fields += f\"cell value has [{', '.join(cols_with_entities[col])}]\\n\"\n",
    "    # fields = find_fields_MYSQL_like(database)\n",
    "    # fields += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(database) + \"\\n\"\n",
    "    # fields += \"Primary_keys = \" + find_primary_keys_MYSQL_like(database)\n",
    "    prompt = (\n",
    "        instruction\n",
    "        + fields\n",
    "        + \"#### Question: \"\n",
    "        + test_sample_text\n",
    "        + \"\\n#### SQLite SQL QUERY\\n\"\n",
    "        + sql\n",
    "        + \"\\n#### SQLite FIXED SQL QUERY\\nSELECT\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def GPT4_debug(chatbot, prompt, model=\"text-davinci-002-render-sha\"):\n",
    "    for data in chatbot.ask(prompt, conversation_id=None, parent_id=None, model=model):\n",
    "        response = data[\"message\"]\n",
    "    GPT4_clear_conversations(chatbot)\n",
    "    return response\n",
    "\n",
    "\n",
    "def GPT4_clear_conversations(chatbot):\n",
    "    chatbot.clear_conversations()\n",
    "\n",
    "\n",
    "spider_schema, spider_primary, spider_foreign = creatiing_schema(DATASET_SCHEMA)\n",
    "val_df = load_data(DATASET)\n",
    "SQL_list = load_sql_txt(BEFORE_SQL)\n",
    "schema_link_list = load_sql_txt(SCHEMA_LINK_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "index: 215\n",
      "{'Airline': ['United Airlines', 'US Airways', 'Delta Airlines', 'Southwest Airlines', 'American Airlines', 'Northwest Airlines', 'Continental Airlines', 'JetBlue Airways', 'Frontier Airlines', 'AirTran Airways'], 'SourceAirport': [' APG', ' ASY', ' CVO', ' ACV', ' AHD', ' AHT', ' ATO', ' ABR', ' ANV', ' MMI'], 'DestAirport': [' ASY', ' APG', ' ACV', ' CVO', ' AHT', ' AHD', ' ABR', ' ATO', ' MMI', ' ANV']}\n",
      "#### For the given question, use the provided cell values to fix the given SQLite SQL QUERY for any issues. If there are any problems, fix them. If there are no issues, return the SQLite SQL QUERY as is.\n",
      "column = [Airline]\n",
      "cell value includes [United Airlines, US Airways, Delta Airlines, Southwest Airlines, American Airlines, Northwest Airlines, Continental Airlines, JetBlue Airways, Frontier Airlines, AirTran Airways]\n",
      "column = [SourceAirport]\n",
      "cell value includes [ APG,  ASY,  CVO,  ACV,  AHD,  AHT,  ATO,  ABR,  ANV,  MMI]\n",
      "column = [DestAirport]\n",
      "cell value includes [ ASY,  APG,  ACV,  CVO,  AHT,  AHD,  ABR,  ATO,  MMI,  ANV]\n",
      "#### Question: How many 'United Airlines' flights go to Airport 'ASY'?\n",
      "#### SQLite SQL QUERY\n",
      "SELECT count(*)  FROM flights  WHERE Airline = 'United Airlines'    AND (DestAirport = 'ASY' OR SourceAirport = 'ASY')\n",
      "#### SQLite FIXED SQL QUERY\n",
      "SELECT\n"
     ]
    }
   ],
   "source": [
    "for ((index, row), SQL, schema_link) in zip(val_df.iterrows(), SQL_list, schema_link_list):\n",
    "    if index  != 215: continue\n",
    "    db = row['db_id']\n",
    "    print('\\nindex:', index)\n",
    "    # 直接空白鍵切割字串，遇到雙引號、單引號，代表entities，此值的前兩個位置就是該table及欄位，取得它然後跑cursor\n",
    "    \n",
    "    cols_have_entities = find_cols_with_entities_by_sql(SQL)\n",
    "    if len(cols_have_entities) == 0:\n",
    "        # print(SQL)\n",
    "        pass\n",
    "    else:\n",
    "        prompt = entities_debuger(row['question'], row['db_id'], SQL, cols_have_entities)\n",
    "        print(prompt)\n",
    "    # print(cols_with_entities)\n",
    "    if index > 300: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sql_string = \"SELECT AirportName  FROM airports  WHERE AirportCode = 'AKO'\"\n",
    "print('\\'AKO\\'' in sql_string.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cursor.execute(\"SELECT * FROM singer LIMIT 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Joe Sharp', 'Netherlands', 'You', '1992', 52, 'F'),\n",
       " (2, 'Timbaland', 'United States', 'Dangerous', '2008', 32, 'T'),\n",
       " (3, 'Justin Brown', 'France', 'Hey Oh', '2013', 29, 'T'),\n",
       " (4, 'Rose White', 'France', 'Sun', '2003', 41, 'F'),\n",
       " (5, 'John Nizinik', 'France', 'Gentleman', '2014', 43, 'T')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'American\n",
      "car_makers.FullName\n"
     ]
    }
   ],
   "source": [
    "sql_string = \"SELECT COUNT(DISTINCT ModelId) FROM model_list JOIN car_makers ON model_list.Maker = car_makers.Id WHERE car_makers.FullName = 'American Motor Company'\"\n",
    "\n",
    "sql_str_list = sql_string.split(' ')\n",
    "for index, str in enumerate(sql_str_list):\n",
    "    if str.startswith('\\''):\n",
    "        print(str)\n",
    "        try:\n",
    "            tb_col = sql_str_list[index-2]\n",
    "            tb = tb_col.split('.')[0]\n",
    "            col = tb_col.split('.')[1]\n",
    "        except:\n",
    "            print(sql_str_list[index-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "键不存在于字典中。\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\"Key1\": \"Value1\", \"KEY2\": \"Value2\", \"kEy3\": \"Value3\"}\n",
    "\n",
    "key_to_find = \"key1\"\n",
    "if key_to_find.lower() in my_dict:\n",
    "    print(\"键存在于字典中。\")\n",
    "else:\n",
    "    print(\"键不存在于字典中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_revchatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
